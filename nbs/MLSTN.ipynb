{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a32b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.layers import TimeDistributed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "#from albumentations.pytorch.transforms import ToTensorV2\n",
    "#from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision import transforms, models\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "from utils import visualize, plot_data\n",
    "from loss.lstn_loss import mlstn_loss\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2296f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/mnt/home/hheat/USERDIR/counting-bench/data'\n",
    "train_images = path + '/images'\n",
    "test_images = path + '/test_images/images'\n",
    "anno = path + '/annotation'\n",
    "#density_maps = path + '/dmaps'\n",
    "sm_train_images = path + '/sm_train_images'\n",
    "sm_test_images = path + '/sm_test_images'\n",
    "sm_dmaps = path + '/sm_dmaps'\n",
    "\n",
    "LOG_PARA = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b70a59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "        ],\n",
    ")\n",
    "\n",
    "def get_train_image_only_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, \n",
    "                                       contrast_limit=0.2, p=0.5),\n",
    "            A.Blur(blur_limit=3,p=0.2),\n",
    "        ],\n",
    "        additional_targets={'image': 'image'}\n",
    "    )\n",
    "\n",
    "# def get_valid_trainsforms():\n",
    "#     return A.Compose(\n",
    "#         [\n",
    "#             #A.Resize(360,640,interpolation=2),\n",
    "#             A.Normalize(mean=mean,std=std,p=1.0,max_pixel_value=1.0),\n",
    "#             ToTensorV2(p=1.0),\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "mean = torch.tensor([0.4939, 0.4794, 0.4583])\n",
    "std = torch.tensor([0.2177, 0.2134, 0.2144])\n",
    "\n",
    "def denormalize(img):\n",
    "    img = img * std[...,None,None] + mean[...,None,None]\n",
    "    img = img.permute(1,2,0).cpu().numpy()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd23cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Video_Counting_Dataset(Dataset):\n",
    "    def __init__(self,path,image_fnames,dmap_folder,\n",
    "                 seq_len=5,gt_folder=None,suffix='jpg',\n",
    "                 tfms=None,mosaic=False,\n",
    "                 crop_size=384,method='train',\n",
    "                 sample=0,segment=5,num_sample=5,downsample=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.path = path\n",
    "        self.image_fnames = image_fnames\n",
    "        \n",
    "        # TODO:\n",
    "        # Be able to get sequences\n",
    "        self.image_fnames = sorted(self.image_fnames,key=self._split_fn)\n",
    "        \n",
    "        self.crop_size = crop_size\n",
    "        if method not in ['train','valid']:\n",
    "            raise Exception('Not Implement')\n",
    "        self.method = method\n",
    "        self.LOG_PARA = LOG_PARA\n",
    "        \n",
    "        self.dmap_folder = path + dmap_folder\n",
    "        self.seq_len = seq_len\n",
    "        self.transforms = tfms\n",
    "        self.mosaic = mosaic\n",
    "        self.gt_folder = path + gt_folder # test purpose\n",
    "        self.sample = sample # 0 is consective, 1 is TSN\n",
    "        self.segment = segment\n",
    "        self.num_sample = num_sample\n",
    "        self.downsample = downsample\n",
    "        self.mean = np.array([0.485, 0.456, 0.406])\n",
    "        self.std = np.array([0.229, 0.224, 0.225])\n",
    "        self.item_tfms = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=self.mean,std=self.std),\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_fnames)\n",
    "    \n",
    "    def _split_fn(self,f):\n",
    "        f = f.split('/')[-1].split('.')[0]\n",
    "        return int(f[3:5]),int(f[-3:])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "            Get a sequence of frames\n",
    "            Return: \n",
    "                frames, shape: seq, h,w,c\n",
    "                dmaps, shape: seq, h,w\n",
    "                gt_points, seq_len of each frame\n",
    "        '''\n",
    "        if self.sample: # TSN sampling\n",
    "            frames,dmaps,fns,gt_points = self._tsn_sampling(idx)\n",
    "        else:\n",
    "            frames,dmaps,fns,gt_points = self._consective_sampling(idx)\n",
    "            \n",
    "        h,w = frames.shape[-2], frames.shape[-1]\n",
    "        \n",
    "        if self.method == 'train':\n",
    "            i,j = self._random_crop(h,w,self.crop_size,self.crop_size)\n",
    "            frames = frames[:,:,i:i+self.crop_size,j:j+self.crop_size]\n",
    "            dmaps = dmaps[:,i:i+self.crop_size,j:j+self.crop_size]\n",
    "            \n",
    "#             import pdb\n",
    "#             pdb.set_trace()\n",
    "            #for idx in range(len(gt_points)):\n",
    "            #    gt_points[idx] = [p_l - [j,i] for p_l in gt_points[idx]]\n",
    "            #    mask = [(p[0]>=0) * (p[0]<self.crop_size) * (p[1]>=0) * (p[1]<self.crop_size) for p in gt_points[idx]]\n",
    "            #    gt_points[idx] = [gt_p[m] for gt_p, m in zip(gt_points[idx],mask) if m]\n",
    "            \n",
    "#             gt_points = gt_points - [j,i]\n",
    "#             mask = (gt_points[:,0] >=0 ) * (gt_points[:,0] <= self.crop_size) * (gt_points[:,1]>=0) * (gt_points[:,1]<=self.crop_size)\n",
    "#             gt_points = gt_points[mask]\n",
    "#             density_map = cv2.resize(density_map,(self.crop_size//self.downsample,self.crop_size//self.downsample))\n",
    "            dmaps = [cv2.resize(dmap,(self.crop_size//self.downsample,self.crop_size//self.downsample)) for dmap in dmaps]\n",
    "            dmaps = np.stack(dmaps)\n",
    "        else:\n",
    "            dmaps = [cv2.resize(dmap,(w//self.downsample,h//self.downsample)) for dmap in dmaps]\n",
    "            dmaps = np.stack(dmaps)\n",
    "        \n",
    "        frames = torch.from_numpy(frames)\n",
    "        dmaps = torch.from_numpy(dmaps)\n",
    "            \n",
    "        if not isinstance(self.transforms,type(None)):\n",
    "            t,ch,h,w = frames.shape\n",
    "            frames = frames.view(t*ch,h,w).permute(1,2,0).numpy()\n",
    "            dmaps = dmaps.permute(1,2,0).numpy()\n",
    "            for tfms in self.transforms:\n",
    "                aug = tfms(**{\n",
    "                    'image': frames,\n",
    "                    'mask': dmaps\n",
    "                })\n",
    "                frames, dmaps = aug['image'], aug['mask']\n",
    "                \n",
    "            frames = torch.from_numpy(frames).permute(2,0,1).view(t,ch,h,w)\n",
    "            dmaps = torch.from_numpy(dmaps).permute(2,0,1)\n",
    "        return frames, dmaps*self.LOG_PARA, fns, gt_points\n",
    "    \n",
    "    def _tsn_sampling(self,idx):\n",
    "        '''\n",
    "        Note:\n",
    "        This method broadly corresponds to the frame sampling technique\n",
    "        introduced in ``Temporal Segment Networks`` at ECCV2016\n",
    "        https://arxiv.org/abs/1608.00859.\n",
    "        \n",
    "        Note: \n",
    "        Minor changes:\n",
    "        1) When frames don't have enough to sample, resample at the given region until we reach segments\n",
    "            pitfall: if last idx is passed, then tsn sampling will duplicate segment times of last frame\n",
    "        2) offset index when we have enough frames\n",
    "        \n",
    "        Args: \n",
    "            idx: call by __getitem__\n",
    "        \n",
    "        Returns:\n",
    "            List of frames sampled in tensor\n",
    "            List of density maps sampled in tensor\n",
    "            List of file names\n",
    "            List of point annotation\n",
    "        '''\n",
    "        frames, d_maps, fns, gt_points = [],[],[],[]\n",
    "        length = self.segment * self.num_sample\n",
    "        start_frame = idx\n",
    "        \n",
    "        end_idx = ((idx // 350) + 1) * 350 - 1\n",
    "        if end_idx > self.__len__() - 1:\n",
    "            end_idx = self.__len__() - 1\n",
    "        # Edge case when we don't have enough to sample, result repeated frames\n",
    "        if start_frame + length > end_idx:\n",
    "            idxs = np.sort(np.random.randint(start_frame,end_idx+1,self.segment))\n",
    "        \n",
    "        # Sample segment times, sampling gap equals to num_sample\n",
    "        else:\n",
    "            end_frame = start_frame + length - 1\n",
    "            idxs = (np.arange(start_frame,end_frame+1,self.num_sample)\n",
    "                    + np.random.randint(self.num_sample,size=self.segment))\n",
    "        for idx in idxs:\n",
    "            fn = self.image_fnames[idx]\n",
    "            image,dmap,points = self._load_one_frame(fn)\n",
    "            frames.append(image)\n",
    "            d_maps.append(dmap)\n",
    "            fns.append(fn)\n",
    "            gt_points.append(points)\n",
    "        return np.stack(frames),np.stack(d_maps),fns,np.array(gt_points)\n",
    "    \n",
    "    def _consective_sampling(self,idx):\n",
    "        '''\n",
    "        Choose consective frames from given positin idx\n",
    "        \n",
    "        Args:\n",
    "            idx: call by __getitem__\n",
    "            \n",
    "        Returns:\n",
    "            List of frames sampled in tensor\n",
    "            List of density maps sampled in tensor\n",
    "            List of file names\n",
    "            List of point annotation\n",
    "        '''\n",
    "        frames, d_maps, fns, gt_points = [],[],[],[]\n",
    "        \n",
    "        end_idx = ((idx // 350) + 1) * 350 - 1\n",
    "        \n",
    "        frame_diff = end_idx - idx\n",
    "        if frame_diff >= self.seq_len:\n",
    "            start_frame = idx\n",
    "        elif frame_dff < self.seq_len:\n",
    "            # random back off when sampling dont have enough samples\n",
    "            idx -= frame_diff\n",
    "            start_frame = round((idx - frame_diff) * np.random.rand())\n",
    "        else:\n",
    "            raise ValueError('start_frame init error...')\n",
    "        for n in range(idx,idx+self.seq_len):\n",
    "                fn = self.image_fnames[n]\n",
    "                image,dmap,points = self._load_one_frame(fn)\n",
    "                frames.append(image)\n",
    "                d_maps.append(dmap)\n",
    "                fns.append(fn)\n",
    "                gt_points.append(points)\n",
    "        \n",
    "        return np.stack(frames),np.stack(d_maps),fns,np.array(gt_points)\n",
    "    \n",
    "    def _load_one_frame(self,fn):\n",
    "        y_fn, p_fn = self._prepare_fn(fn)\n",
    "        image = cv2.imread(fn)\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image = image/255.\n",
    "        image = self.item_tfms(image)\n",
    "        d_map, points = self._get_gts(y_fn,p_fn)\n",
    "        d_map = torch.from_numpy(d_map)\n",
    "        return image, d_map, points\n",
    "    \n",
    "    def _prepare_fn(self,fn):\n",
    "        file_name = fn.split('/')[-1].split('.')[0]\n",
    "        y_fn = self.dmap_folder + '/' + file_name + '.npy'\n",
    "        p_fn = self.gt_folder + '/' + file_name[3:] + '.mat'\n",
    "        return y_fn, p_fn\n",
    "    \n",
    "    def _get_gts(self,y_fn,p_fn):\n",
    "        d_map = np.load(y_fn,allow_pickle=True)\n",
    "        if not self.gt_folder:\n",
    "            return (None,0)\n",
    "        test_data = loadmat(p_fn)\n",
    "        points = test_data['annotation'].astype(int)\n",
    "        points[:,0] = points[:,0] / 2720 * 1360\n",
    "        points[:,1] = points[:,1] / 1530 * 784\n",
    "        return d_map, points\n",
    "    \n",
    "    def _random_crop(self, im_h, im_w, crop_h, crop_w):\n",
    "        res_h = im_h - crop_h\n",
    "        res_w = im_w - crop_w\n",
    "        i = random.randint(0, res_h)\n",
    "        j = random.randint(0, res_w)\n",
    "        return i, j\n",
    "    \n",
    "    def _normalize(self,img):\n",
    "        img -= self.mean[None,None,...]\n",
    "        img /= self.std[None,None,...]\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfa6e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Video_Counting_Dataset(path=path, image_fnames=glob(sm_train_images+'/*.jpg'),\n",
    "                                       dmap_folder='/sm_dmap_lstn',\n",
    "                                       gt_folder='/annotation',\n",
    "                                       #tfms=[get_train_transforms(),get_train_image_only_transforms()],\n",
    "                                       sample=1, method='valid',downsample=8\n",
    ")\n",
    "valid_dataset = Video_Counting_Dataset(path=path, image_fnames=glob(sm_test_images+'/*.jpg'),\n",
    "                                       dmap_folder='/sm_dmap_lstn',\n",
    "                                       gt_folder='/annotation',\n",
    "                                       sample=1, method='valid',downsample=8 #, segment=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4655c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs,dmaps,fns,gt_points = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af9abaa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 784, 1360])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24f7754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainGlobalConfig:\n",
    "    num_workers = 16\n",
    "    batch_size = 8\n",
    "    n_epochs = 1 \n",
    "    lr = 0.0002\n",
    "\n",
    "    folder = 'test_delete'\n",
    "    downsample = 8\n",
    "    split_num = 2\n",
    "\n",
    "    # -------------------\n",
    "    verbose = True\n",
    "    verbose_step = 1\n",
    "    # -------------------\n",
    "\n",
    "    # --------------------\n",
    "    step_scheduler = True  # do scheduler.step after optimizer.step\n",
    "    validation_scheduler = False  # do scheduler.step after validation stage loss\n",
    "\n",
    "    SchedulerClass = torch.optim.lr_scheduler.OneCycleLR\n",
    "    scheduler_params = dict(\n",
    "        max_lr=1e-4,\n",
    "        #total_steps = len(train_dataset) // 4 * n_epochs, # gradient accumulation\n",
    "        epochs=n_epochs,\n",
    "        steps_per_epoch=int(len(train_dataset) / batch_size),\n",
    "        pct_start=0.2,\n",
    "        anneal_strategy='cos', \n",
    "        final_div_factor=10**5\n",
    "    )\n",
    "    \n",
    "#     SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "#     scheduler_params = dict(\n",
    "#         mode='min',\n",
    "#         factor=0.5,\n",
    "#         patience=1,\n",
    "#         verbose=False, \n",
    "#         threshold=0.0001,\n",
    "#         threshold_mode='abs',\n",
    "#         cooldown=0, \n",
    "#         min_lr=1e-8,\n",
    "#         eps=1e-08\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb712a21",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f4b6289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "class MLSTN(nn.Module):\n",
    "    def __init__(self, input_size=(72, 112)):\n",
    "        super(MLSTN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.vgg16 = VGG16()\n",
    "        #self.stn = STN((int(self.input_size[0]//4), int(self.input_size[1]//4)))\n",
    "        self.stn = STN((input_size[0]//8, input_size[1]//8))\n",
    "        self.stnt = STN((input_size[0]//2, input_size[1]//2))\n",
    "    \n",
    "    @torch.cuda.amp.autocast()\n",
    "    def forward(self, x, setname='train'):\n",
    "        \"\"\"\n",
    "        :param x: frames t0, t1, t2 with size (B, Frames, C, H, W)=(B, 3, 3, 360, 640)\n",
    "        :return:\n",
    "                multi_maps: density maps at time t0, t1, t2 from VGG-16 with size (B, 3, 1, 90, 160)\n",
    "                map_t3: density maps at time t3 with size (B, 1, 90, 160)\n",
    "        \"\"\"\n",
    "        # each frame is consecutively put into the vgg16 net\n",
    "        for i in range(x.shape[1]):\n",
    "            maptemp = self.vgg16(x[:, i, :, :, :])\n",
    "            if i == 0:\n",
    "                multi_maps = maptemp.unsqueeze(1)\n",
    "            else:\n",
    "                multi_maps = torch.cat((multi_maps, maptemp.unsqueeze(1)), dim=1)\n",
    "\n",
    "        # concatenate multi maps from t0, t1, t2 by squeezing dim2\n",
    "        map_t3 = self.stn(multi_maps[:, :, 0, :, :])\n",
    "        return multi_maps, map_t3\n",
    "\n",
    "\n",
    "\n",
    "class STN(nn.Module):\n",
    "    def __init__(self, h_w_size=(90, 80)):\n",
    "        super(STN, self).__init__()\n",
    "        # Spatial transformer localization-network\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=7),     # input channels are modified to 3\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        #print(f'hw: {h_w_size}')\n",
    "\n",
    "        # calculate the input size for linear layer\n",
    "        h = (h_w_size[0] - 7) + 1\n",
    "        h = int((h-2)/2)+1\n",
    "        h = (h - 5) + 1\n",
    "        h = int((h - 2) / 2) + 1\n",
    "\n",
    "        w = (h_w_size[1] - 7) + 1\n",
    "        w = int((w-2)/2)+1\n",
    "        w = (w - 5) + 1\n",
    "        w = int((w - 2) / 2) + 1\n",
    "        \n",
    "        #print(f'hw_after: {h}, {w}')\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(10 * h * w, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "        self.output_layer = nn.Conv2d(3, 1, kernel_size=1)\n",
    "\n",
    "    @torch.cuda.amp.autocast()\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: block of output from VGG16 at time t with size (B, 3, 90, 160)\n",
    "        :return: output size (B, 1, 90, 160)\n",
    "        \"\"\"\n",
    "        #print(f'x, {x.shape}')\n",
    "        xs = self.localization(x)\n",
    "        #print(f'xs, {xs.shape}')\n",
    "        xs = xs.view(x.shape[0], -1)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self, load_weights=False, fix_weights=True):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.seen = 0\n",
    "        self.frontend_feat = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512]\n",
    "        self.backend_feat  = [512, 512, 512,256,128,64]\n",
    "        self.frontend = make_layers(self.frontend_feat)\n",
    "        self.backend = make_layers(self.backend_feat,in_channels = 512,dilation = True)\n",
    "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        if not load_weights:\n",
    "            mod = models.vgg16(pretrained = True)\n",
    "            self._initialize_weights()\n",
    "            self.frontend.load_state_dict(mod.features[0:23].state_dict())\n",
    "    @torch.cuda.amp.autocast()\n",
    "    def forward(self,x):\n",
    "        x = self.frontend(x)\n",
    "        x = self.backend(x)\n",
    "        x = self.output_layer(x)\n",
    "        #x = F.upsample(x,scale_factor=2)\n",
    "        #print(f'vgg_output: {x.shape}')\n",
    "        return x\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def make_layers(cfg, in_channels=3, batch_norm=False, dilation=False):\n",
    "    if dilation:\n",
    "        d_rate = 2\n",
    "    else:\n",
    "        d_rate = 1\n",
    "    layers = []\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate, dilation=d_rate)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd27d737",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2da06e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SummaryLoss_MLSTN(preds_t012, preds_t3, gts, imgs):\n",
    "    return mlstn_loss(preds_t012, preds_t3, gts, imgs, lamda=0.001, beta=30)\n",
    "\n",
    "def MSELoss_MCNN(preds,targs):\n",
    "    return nn.MSELoss()(preds,targs)\n",
    "\n",
    "def MAELoss_MCNN(preds,targs,upsample):\n",
    "    return nn.L1Loss()((preds/LOG_PARA).sum(dim=[-1,-2])*upsample*upsample, (targs/LOG_PARA).sum(dim=[-1,-2])*upsample*upsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7d976a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2862dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#opt_level ='O1' # apex\n",
    "\n",
    "class Fitter:\n",
    "    \n",
    "    def __init__(self, model, device, config):\n",
    "        self.config = config\n",
    "        self.epoch = 0\n",
    "\n",
    "        self.base_dir = f'/mnt/home/zpengac/USERDIR/count/drone_benchmark/{config.folder}'\n",
    "        if not os.path.exists(self.base_dir):\n",
    "            os.makedirs(self.base_dir)\n",
    "        \n",
    "        self.log_path = f'{self.base_dir}/log.txt'\n",
    "        self.best_summary_loss = 10**5\n",
    "\n",
    "        self.model = model\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "          # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "            #self.model = nn.DataParallel(self.model)\n",
    "        self.device = device\n",
    "\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ] \n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n",
    "        \n",
    "        #self.model, self.optimizer = amp.initialize(self.model,self.optimizer,opt_level=opt_level) # apex\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n",
    "        self.criterion = SummaryLoss_MLSTN\n",
    "        self.metric = MAELoss_MCNN\n",
    "        self.log(f'Fitter prepared. Device is {self.device}')\n",
    "        \n",
    "        # self.iters_to_accumulate = 4 # gradient accumulation\n",
    "\n",
    "    def fit(self, train_loader, validation_loader):\n",
    "        for e in range(self.config.n_epochs):\n",
    "            if self.config.verbose:\n",
    "                lr = self.optimizer.param_groups[0]['lr']\n",
    "                timestamp = datetime.utcnow().isoformat()\n",
    "                self.log(f'\\n{timestamp}\\nLR: {lr}')\n",
    "\n",
    "            t = time.time()\n",
    "            summary_loss, mae_loss = self.train_one_epoch(train_loader)\n",
    "\n",
    "            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.8f}, time: {(time.time() - t):.5f}')\n",
    "            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, mae_loss: {mae_loss.avg:.8f}, time: {(time.time() - t):.5f}')\n",
    "            self.save(f'{self.base_dir}/last-checkpoint.bin')\n",
    "\n",
    "            t = time.time()\n",
    "            summary_loss, mae_loss = self.validation(validation_loader)\n",
    "\n",
    "            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.8f}, time: {(time.time() - t):.5f}')\n",
    "            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, mae_loss: {mae_loss.avg:.8f}, time: {(time.time() - t):.5f}')\n",
    "            if summary_loss.avg < self.best_summary_loss:\n",
    "                self.best_summary_loss = summary_loss.avg\n",
    "                self.model.eval()\n",
    "                self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n",
    "                for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-3]:\n",
    "                    os.remove(path)\n",
    "\n",
    "            if self.config.validation_scheduler:\n",
    "                self.scheduler.step(metrics=summary_loss.avg)\n",
    "\n",
    "            self.epoch += 1\n",
    "\n",
    "    def validation(self, val_loader):\n",
    "        self.model.eval()\n",
    "        summary_loss = AverageMeter()\n",
    "        mae_loss = AverageMeter()\n",
    "        t = time.time()\n",
    "        for step, (images, density_maps, fns, gt_pts) in enumerate(val_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    print(\n",
    "                        f'Val Step {step}/{len(val_loader)}, ' + \\\n",
    "                        f'summary_loss: {summary_loss.avg:.8f}, ' + \\\n",
    "                        f'mae_loss: {mae_loss.avg:.8f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            \n",
    "                \n",
    "\n",
    "                #preds = self.model(images)\n",
    "            with torch.no_grad():\n",
    "                batch_size = images.shape[0]\n",
    "                h_split,w_split = (\n",
    "                    images.shape[-2]//self.config.split_num,\n",
    "                    images.shape[-1]//self.config.split_num #bs,T,c,h,w\n",
    "                )\n",
    "                #images = images.to(self.device).float()\n",
    "                images = images.cuda()\n",
    "                stacked_images = torch.stack([images[:,i:i+3,...] for i in range(3)]).transpose(0,1).contiguous()\n",
    "                #images = images.transpose(1,2)\n",
    "                density_maps = density_maps.cuda()\n",
    "                #density_maps = density_maps.to(self.device).float()\n",
    "                \n",
    "                \n",
    "                \n",
    "                with torch.cuda.amp.autocast(): #native fp16\n",
    "                    ### added\n",
    "                    #bs,t,c,h,w = images.shape\n",
    "                    preds_t012, preds_t3 = self.model(stacked_images)\n",
    "                    preds = torch.cat([preds_t012[:,0,0:1,...], preds_t012[:,1,0:1,...], preds_t012[:,2,...]], dim=1)\n",
    "                    #preds = preds.view(bs,t,h//self.config.downsample,w//self.config.downsample)\n",
    "                    \n",
    "#                     preds = torch.zeros([bs,t,h,w])\n",
    "#                     for r in range(self.config.split_num):\n",
    "#                         for c in range(self.config.split_num):\n",
    "#                             preds[:,:,h_split*r:h_split*(r+1),w_split*c:w_split*(c+1)] = (\n",
    "#                                 self.model(images[:,:,:,h_split*r:h_split*(r+1),w_split*c:w_split*(c+1)]).view(8,1,392,-1)#.view(4,1,765,-1)\n",
    "#                             )\n",
    "                    \n",
    "                    loss = self.criterion(preds_t012, preds_t3, density_maps, images)\n",
    "                    metric_loss = self.metric(preds.squeeze(),density_maps,self.config.downsample)\n",
    "                mae_loss.update(metric_loss.detach().item(),batch_size)\n",
    "                summary_loss.update(loss.detach().item(), batch_size)\n",
    "                \n",
    "            #if step == 10:\n",
    "            #    break\n",
    "\n",
    "        return summary_loss, mae_loss\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        summary_loss = AverageMeter()\n",
    "        mae_loss = AverageMeter()\n",
    "        t = time.time()\n",
    "        for step, (images, density_maps, fns, gt_pts) in enumerate(train_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    print(\n",
    "                        f'Train Step {step}/{len(train_loader)}, ' + \\\n",
    "                        f'summary_loss: {summary_loss.avg:.8f}, ' + \\\n",
    "                        f'mae_loss: {mae_loss.avg:.8f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            \n",
    "            #images = images.to(self.device).float()\n",
    "            images = images.cuda().float()\n",
    "            stacked_images = torch.stack([images[:,i:i+3,...] for i in range(3)]).transpose(0,1).contiguous()\n",
    "            #images = images.transpose(1,2)\n",
    "            batch_size = images.shape[0]\n",
    "            #density_maps = density_maps.to(self.device).float()\n",
    "            density_maps = density_maps.cuda().float()\n",
    "            \n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            with torch.cuda.amp.autocast(): #native fp16\n",
    "                #print(images.shape)\n",
    "                preds_t012, preds_t3 = self.model(stacked_images)\n",
    "                loss = self.criterion(preds_t012, preds_t3, density_maps, images)\n",
    "                preds = torch.cat([preds_t012[:,0,0:1,...], preds_t012[:,1,0:1,...], preds_t012[:,2,...]], dim=1)\n",
    "                metric_loss = self.metric(preds.squeeze().cpu().detach(),density_maps.cpu().detach(),self.config.downsample)\n",
    "            self.scaler.scale(loss).backward()\n",
    "            \n",
    "            # loss = loss / self.iters_to_accumulate # gradient accumulation\n",
    "            \n",
    "#             with amp.scale_loss(loss,self.optimizer) as scaled_loss: # apex\n",
    "#                 scaled_loss.backward()\n",
    "            #loss.backward()\n",
    "\n",
    "            \n",
    "            mae_loss.update(metric_loss.detach().item(),batch_size)\n",
    "            summary_loss.update(loss.cpu().detach().item(), batch_size)\n",
    "            \n",
    "            #self.optimizer.step()\n",
    "            self.scaler.step(self.optimizer) # native fp16\n",
    "            \n",
    "            if self.config.step_scheduler:\n",
    "                self.scheduler.step()\n",
    "            \n",
    "            self.scaler.update() #native fp16\n",
    "                \n",
    "                \n",
    "#             if (step+1) % self.iters_to_accumulate == 0: # gradient accumulation\n",
    "\n",
    "#                 self.optimizer.step()\n",
    "#                 self.optimizer.zero_grad()\n",
    "\n",
    "#                 if self.config.step_scheduler:\n",
    "#                     self.scheduler.step()\n",
    "            #if step == 10:\n",
    "            #    break\n",
    "\n",
    "        return summary_loss, mae_loss\n",
    "    \n",
    "    def save(self, path):\n",
    "        #self.model.cpu()\n",
    "        self.model.eval()\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_summary_loss': self.best_summary_loss,\n",
    "            'epoch': self.epoch,\n",
    "            #'amp': amp.state_dict() # apex\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.best_summary_loss = checkpoint['best_summary_loss']\n",
    "        self.epoch = checkpoint['epoch'] + 1\n",
    "        \n",
    "    def log(self, message):\n",
    "        if self.config.verbose:\n",
    "            print(message)\n",
    "        with open(self.log_path, 'a+') as logger:\n",
    "            logger.write(f'{message}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "658e3180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    frames, dmaps, fns, gt_points = zip(*batch)\n",
    "    return torch.stack(frames), torch.stack(dmaps), fns, gt_points\n",
    "\n",
    "def run_training():\n",
    "    device = torch.device('cuda:0')\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=RandomSampler(train_dataset),\n",
    "        #sampler=train_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size=TrainGlobalConfig.batch_size//4,\n",
    "        num_workers=TrainGlobalConfig.num_workers//2,\n",
    "        shuffle=False,\n",
    "        sampler=SequentialSampler(valid_dataset),\n",
    "        #sampler=val_sampler,\n",
    "        pin_memory=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n",
    "    #fitter.load(f'{fitter.base_dir}/last-checkpoint.bin')\n",
    "    fitter.fit(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19bfabfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLSTN(input_size=(784,1360)).cuda()\n",
    "net = nn.DataParallel(net)\n",
    "net = TimeDistributed(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76825f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    if not param.is_leaf:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bddc68cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dd9082f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 8 GPUs!\n",
      "Fitter prepared. Device is cuda:0\n",
      "\n",
      "2021-08-07T08:03:01.908677\n",
      "LR: 4.000000000000002e-06\n",
      "[RESULT]: Train. Epoch: 0, summary_loss: 0.02655550, time: 1561.41193time: 1559.61922\n",
      "[RESULT]: Train. Epoch: 0, mae_loss: 33.25415221, time: 1561.42217\n",
      "[RESULT]: Val. Epoch: 0, summary_loss: 0.01595918, time: 449.061091, time: 448.57724\n",
      "[RESULT]: Val. Epoch: 0, mae_loss: 12.49407421, time: 449.07180\n"
     ]
    }
   ],
   "source": [
    "run_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94e3a7f",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3167e213",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_net = MLSTN(input_size=(784,1360)).cuda()\n",
    "test_net = nn.DataParallel(test_net)\n",
    "test_net = TimeDistributed(test_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4e740a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeDistributed(DataParallel(\n",
       "  (module): MLSTN(\n",
       "    (vgg16): VGG16(\n",
       "      (frontend): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (8): ReLU(inplace=True)\n",
       "        (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (11): ReLU(inplace=True)\n",
       "        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (13): ReLU(inplace=True)\n",
       "        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (15): ReLU(inplace=True)\n",
       "        (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (18): ReLU(inplace=True)\n",
       "        (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (20): ReLU(inplace=True)\n",
       "        (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (22): ReLU(inplace=True)\n",
       "      )\n",
       "      (backend): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "        (7): ReLU(inplace=True)\n",
       "        (8): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "        (9): ReLU(inplace=True)\n",
       "        (10): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "        (11): ReLU(inplace=True)\n",
       "      )\n",
       "      (output_layer): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (stn): STN(\n",
       "      (localization): Sequential(\n",
       "        (0): Conv2d(3, 8, kernel_size=(7, 7), stride=(1, 1))\n",
       "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "      (fc_loc): Sequential(\n",
       "        (0): Linear(in_features=8190, out_features=32, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=32, out_features=6, bias=True)\n",
       "      )\n",
       "      (output_layer): Conv2d(3, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (stnt): STN(\n",
       "      (localization): Sequential(\n",
       "        (0): Conv2d(3, 8, kernel_size=(7, 7), stride=(1, 1))\n",
       "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "      (fc_loc): Sequential(\n",
       "        (0): Linear(in_features=156040, out_features=32, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=32, out_features=6, bias=True)\n",
       "      )\n",
       "      (output_layer): Conv2d(3, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(f'/mnt/home/zpengac/USERDIR/count/drone_benchmark/MLSTN-8.7-NoCrop/best-checkpoint-008epoch.bin')\n",
    "test_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "test_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f64190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    frames, dmaps, fns, gt_points = zip(*batch)\n",
    "    return torch.stack(frames), torch.stack(dmaps), fns, gt_points\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TrainGlobalConfig.batch_size,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    num_workers=TrainGlobalConfig.num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=TrainGlobalConfig.batch_size,\n",
    "    num_workers=TrainGlobalConfig.num_workers,\n",
    "    shuffle=False,\n",
    "    sampler=SequentialSampler(valid_dataset),\n",
    "    pin_memory=False,\n",
    "    collate_fn=collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5116f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, dmaps, fns, gt_points = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c8b39d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 5, 3, 784, 1360])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3af861d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 3, 3, 784, 1360])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_imgs = torch.stack([imgs[:,i:i+3,...] for i in range(3)]).transpose(0,1).contiguous()\n",
    "stacked_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52552380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 3, 1, 98, 170])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_t0, preds_t1_blocks = model(stacked_imgs.cuda())\n",
    "preds_t0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb280533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 1, 98, 170])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_t0[:,2,...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09cf0d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as pnsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb3a8e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 8min 53s, sys: 19min 44s, total: 1h 28min 37s\n",
      "Wall time: 7min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pre_count = []\n",
    "gt_count = []\n",
    "gt_points = []\n",
    "avg_ssim = AverageMeter()\n",
    "avg_pnsr = AverageMeter()\n",
    "for step, (imgs, dmapss, fns, points) in enumerate(val_loader):\n",
    "    with torch.no_grad():\n",
    "        imgs = imgs.cuda().float()\n",
    "        stacked_imgs = torch.stack([imgs[:,i:i+3,...] for i in range(3)]).transpose(0,1).contiguous()\n",
    "        predss, _ = test_net(stacked_imgs)\n",
    "        predss = torch.cat([predss[:,0,0:1,...], predss[:,1,0:1,...], predss[:,2,...]], dim=1)\n",
    "        predss = predss / LOG_PARA * (TrainGlobalConfig.downsample**2)\n",
    "    dmapss = dmapss / LOG_PARA * (TrainGlobalConfig.downsample**2)\n",
    "    \n",
    "    for i in range(5):\n",
    "        preds = predss[:,i,...]\n",
    "        dmaps = dmapss[:,i,...]\n",
    "        for pred, dmap in zip(preds, dmaps):\n",
    "            pred_array = pred.detach().cpu().numpy().squeeze()\n",
    "            dmap_array = dmap.detach().cpu().numpy().squeeze()\n",
    "            avg_ssim.update(ssim(dmap_array, pred_array, data_range=dmap_array.max()-dmap_array.min()))\n",
    "            avg_pnsr.update(pnsr(dmap_array, pred_array, data_range=dmap_array.max()-dmap_array.min()))\n",
    "    \n",
    "    pre_count.extend(predss.sum(dim=[-1,-2]).detach().cpu().numpy())\n",
    "    gt_count.extend(dmapss.sum(dim=[-1,-2]).detach().cpu().numpy())\n",
    "    \n",
    "    gt_p = []\n",
    "    for p in points:\n",
    "        gt_p.append(len(p))\n",
    "    gt_points.extend(gt_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91edd6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_count_new = [pre.reshape(-1) for pre in pre_count]\n",
    "pre_count_new = np.concatenate(pre_count_new)\n",
    "gt_count_new = [gt.reshape(-1) for gt in gt_count]\n",
    "gt_count_new = np.concatenate(gt_count_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42fd7a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(pre_count_new,gt_count_new)\n",
    "mse = mean_squared_error(pre_count_new,gt_count_new)\n",
    "nae = mae * len(pre_count_new) / np.sum(gt_count_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca3be00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Paras: 21.525705\n",
      "MAE: 11.550806644699527, MSE: 524.2172837110822, NAE: 0.08835654015645894\n",
      "SSIM: 0.9436800545593427, PNSR: 30.891791333937523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/zpengac/.Miniconda3/envs/f4774e49c9ffe87fb0928ec97f8ff682/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def count_parameters_in_MB(model):\n",
    "    return np.sum(np.prod(v.size()) for name, v in model.named_parameters() if \"auxiliary\" not in name) / 1e6\n",
    "\n",
    "print(f'#Paras: {count_parameters_in_MB(test_net)}')\n",
    "print(f'MAE: {mae}, MSE: {mse}, NAE: {nae}')\n",
    "print(f'SSIM: {avg_ssim.avg}, PNSR: {avg_pnsr.avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ed9dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
