{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43292e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from counTR import CounTR\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "\n",
    "\n",
    "# import torch.multiprocessing as mp\n",
    "# from torch.nn import DataParallel\n",
    "\n",
    "from glob import glob\n",
    "from scipy.io import loadmat\n",
    "\n",
    "\n",
    "import timm\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e6c56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/mnt/home/zpengac/USERDIR/count/drone_dataset'\n",
    "images = path + '/rf_image_vehicle'\n",
    "density_maps = path + '/rf_GT_vehicle'\n",
    "\n",
    "# sm_train_images = path + '/sm_train_images'\n",
    "# sm_test_images = path + '/sm_test_images'\n",
    "# sm_dmaps = path + '/sm_dmaps'\n",
    "\n",
    "\n",
    "LOG_PARA = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b563810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            #A.Resize(360,640,interpolation=2),\n",
    "            #A.RandomSizedCrop(min_max_height=(409, 512), height=409, width=512, p=1.0),\n",
    "            #A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=1.0),\n",
    "        ],\n",
    "        #additional_targets={'image': 'image','image1': 'image'}\n",
    "        #keypoint_params = A.KeypointParams(format='xy')\n",
    ")\n",
    "\n",
    "def get_train_image_only_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            #A.Resize(360,640),\n",
    "#             A.OneOf([\n",
    "#                 A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
    "#                                      val_shift_limit=0.2, p=0.9),\n",
    "#                 A.RandomBrightnessContrast(brightness_limit=0.2, \n",
    "#                                            contrast_limit=0.2, p=0.9),\n",
    "#             ],p=0.9),\n",
    "#             A.Blur(blur_limit=3,p=0.2),\n",
    "            A.Normalize(p=1.0,max_pixel_value=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        additional_targets={'image': 'image'}\n",
    "    )\n",
    "\n",
    "def get_valid_trainsforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            #A.Resize(360,640,interpolation=2),\n",
    "            A.Normalize(p=1.0,max_pixel_value=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "mean = torch.tensor([0.38868062, 0.38568735, 0.39457315])\n",
    "std = torch.tensor([0.221865, 0.23096273, 0.2210397])\n",
    "\n",
    "def denormalize(img):\n",
    "    img = img * std[...,None,None] + mean[...,None,None]\n",
    "    img = img.permute(1,2,0).cpu().numpy()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb13cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Counting_Dataset(Dataset):\n",
    "    def __init__(self,path,image_fnames,dmap_folder,gt_folder=None,transforms=None,mosaic=False,downsample=4):\n",
    "        '''\n",
    "            path: root path \n",
    "            image_fnames: path of images\n",
    "            dmap_folder: density map folder, eg: /dmap\n",
    "            gt_folder: gt folder, currently set to visdrone xml format, modify _get_gt_data() if needed\n",
    "            transforms: iteratable, can be tuple / list ... etc\n",
    "            mosaic: mix up image and density map to form a new image, set to false by default\n",
    "            downsample: resize dmap\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.image_fnames = image_fnames\n",
    "        self.dmap_folder = path + dmap_folder\n",
    "        self.transforms = transforms\n",
    "        self.mosaic = mosaic\n",
    "        self.downsample = downsample\n",
    "        self.gt_folder = gt_folder # test purpose\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_fnames)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        image_id = self.image_fnames[idx]\n",
    "        \n",
    "        if self.mosaic and random.randint(0,1) < 0.5:\n",
    "            image, density_map, gt_points = self._load_mosaic_image_and_density_map(idx)\n",
    "        else:\n",
    "            image, density_map, gt_points = self._load_image_and_density_map(idx)\n",
    "        \n",
    "        h,w = image.shape[0]//self.downsample, image.shape[1]//self.downsample\n",
    "        image = cv2.resize(image,(w, h))\n",
    "        density_map = cv2.resize(density_map,(w//(self.downsample*2),h//(self.downsample*2)))#,interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        # Warning: doesn't work for cutout, uncommet transform and make fix code to enable cutout\n",
    "        # Reason: cutout doesn't apply to mask, so mask must be image. check 01a bottom for code\n",
    "        if self.transforms:\n",
    "            for tfms in self.transforms:\n",
    "                aug = tfms(**{\n",
    "                    'image': image,\n",
    "                    'mask': density_map,\n",
    "                    #'keypoints': gt_points\n",
    "                })\n",
    "                #image, density_map, gt_points = aug['image'], aug['mask'], aug['keypoints']\n",
    "                image, density_map = aug['image'], aug['mask'] # issue with previous keypoints (albumentation?)\n",
    "        \n",
    "        \n",
    "        return image, density_map, image_id, gt_points\n",
    "        \n",
    "    \n",
    "    def _get_dmap_name(self,fn):\n",
    "        mask_name = fn.split('/')[-1].split('.')[0]\n",
    "        mask_path = self.dmap_folder + '/' + mask_name + '.npy'\n",
    "        return mask_path\n",
    "    \n",
    "    def _load_image_and_density_map(self,idx):\n",
    "        image_fname = self.image_fnames[idx]\n",
    "        dmap_fname = self._get_dmap_name(image_fname)\n",
    "        image = cv2.imread(image_fname)\n",
    "        d_map = np.load(dmap_fname,allow_pickle=True)\n",
    "        d_map = d_map.squeeze()\n",
    "        pad_h = 0\n",
    "        pad_w = 0\n",
    "        if image.shape[0] < 448:\n",
    "            pad_h = (448 - image.shape[0]) // 2\n",
    "        if image.shape[1] < 448:\n",
    "            pad_w = (448 - image.shape[1]) // 2\n",
    "        image = cv2.copyMakeBorder(image, pad_h, pad_h, pad_w, pad_w, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "        d_map = cv2.copyMakeBorder(d_map, pad_h, pad_h, pad_w, pad_w, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image = image/255.\n",
    "        \n",
    "        #sanity check gt\n",
    "        _, points = self._get_gt_data(idx)\n",
    "        # end sanity check\n",
    "        \n",
    "        return image, d_map, points\n",
    "    \n",
    "    def _load_mosaic_image_and_density_map(self,idx):\n",
    "        image_1, dmap_1, points_1 = self._load_image_and_density_map(idx)\n",
    "        while True:\n",
    "            idx_2 = random.randint(0,len(self.image_fnames)-1)\n",
    "            if idx != idx_2:\n",
    "                break\n",
    "        image_2, dmap_2, points_2 = self._load_image_and_density_map(idx_2)\n",
    "        \n",
    "        imsize = min(*image_1.shape[:2])\n",
    "        xc,yc = [int(random.uniform(imsize*0.4,imsize*0.6)) for _ in range(2)]\n",
    "        h,w = image_1.shape[0], image_1.shape[1]\n",
    "\n",
    "        pos = random.randint(0,1)\n",
    "        if pos == 0: #top left\n",
    "            x1a,y1a,x2a,y2a = 0,0,xc,yc # img_1\n",
    "            x1b,y1b,x2b,y2b = w-xc,h-yc,w,h # img_2\n",
    "        elif pos == 1: # top right\n",
    "            x1a,y1a,x2a,y2a = w-xc,0,w,yc\n",
    "            x1b,y1b,x2b,y2b = 0,h-yc,xc,h\n",
    "        elif pos == 2: # bottom left\n",
    "            x1a,y1a,x2a,y2a = 0,h-yc,xc,h\n",
    "            x1b,y1b,x2b,y2b = w-xc,0,w,yc\n",
    "        elif pos == 3: # bottom right\n",
    "            x1a,y1a,x2a,y2a = w-xc,h-yc,w,h\n",
    "            x1b,y1b,x2b,y2b = 0,0,xc,yc\n",
    "        \n",
    "        new_image = image_1.copy()\n",
    "        new_dmap = dmap_1.copy()\n",
    "        new_image[y1a:y2a,x1a:x2a] = image_2[y1b:y2b,x1b:x2b]\n",
    "        new_dmap[y1a:y2a,x1a:x2a] = dmap_2[y1b:y2b,x1b:x2b]\n",
    "        \n",
    "        #TODO: sanity check to see generate gt\n",
    "        \n",
    "        new_gt_points = self._get_mixed_gt_points(points_1,points_2,(x1a,y1a,x2a,y2a),(x1b,y1b,x2b,y2b),(h,w))\n",
    "        \n",
    "        return new_image, new_dmap, new_gt_points\n",
    "    \n",
    "    '''\n",
    "    The follow section blocks are for sanity check \n",
    "    to compare dmap.sum() with gt points\n",
    "    remove if needed\n",
    "    '''\n",
    "    def _get_mixed_gt_points(self,points_1,points_2,img_1_loc, img_2_loc,img_shape):\n",
    "#         fn_1, points_1 = self._get_gt_data(idx_1)\n",
    "#         fn_2, points_2 = self._get_gt_data(idx_2)\n",
    "        x1a,y1a,x2a,y2a = img_1_loc\n",
    "        x1b,y1b,x2b,y2b = img_2_loc\n",
    "        h,w = img_shape\n",
    "        \n",
    "        result_boxes = []\n",
    "        result_boxes.append(points_2)\n",
    "        result_boxes = np.concatenate(result_boxes,0)\n",
    "        padw = x1a-x1b\n",
    "        pady = y1a-y1b\n",
    "\n",
    "        result_boxes[:,0] += padw\n",
    "        result_boxes[:,1] += pady\n",
    "\n",
    "        np.clip(result_boxes[:,0],0,w,out=result_boxes[:,0])\n",
    "        np.clip(result_boxes[:,1],0,h,out=result_boxes[:,1])\n",
    "        result_boxes = result_boxes.astype(np.int32)\n",
    "\n",
    "        result_boxes = result_boxes[np.where(result_boxes[:,0] * result_boxes[:,1] > 0)]\n",
    "        result_boxes = result_boxes[np.where(result_boxes[:,0] < w)]\n",
    "        result_boxes = result_boxes[np.where(result_boxes[:,1] < h)]\n",
    "        \n",
    "        boxes = []\n",
    "        for (x,y) in points_1:\n",
    "            if x >= x1a and x <= x2a and y >= y1a and y <= y2a:\n",
    "                continue\n",
    "            else:\n",
    "                boxes.append((x,y))\n",
    "        if len(boxes) == 0:\n",
    "            return result_boxes\n",
    "        return np.concatenate((boxes, result_boxes),axis=0)\n",
    "    \n",
    "    def _get_gt_data(self,idx):\n",
    "        if not self.gt_folder:\n",
    "            return (None,0)\n",
    "        fn = self.image_fnames[idx]\n",
    "        anno_path = self.path + self.gt_folder + '/' + fn.split('/')[-1].split('.')[0] + '.mat'\n",
    "        test_data = loadmat(anno_path)\n",
    "        points = test_data['annotation'].astype(int)\n",
    "        return fn, points\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ADD LOG_PARA to density map\n",
    "\n",
    "class Crop_Dataset(Counting_Dataset):\n",
    "    def __init__(self,path,image_fnames,dmap_folder,gt_folder=None,transforms=None,mosaic=False,downsample=4,crop_size=512,method='train'):\n",
    "        super().__init__(path,image_fnames,dmap_folder,gt_folder,transforms,mosaic,downsample)\n",
    "        self.crop_size = crop_size\n",
    "        if method not in ['train','valid']:\n",
    "            raise Exception('Not Implement')\n",
    "        self.method = method\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        fn = self.image_fnames[idx]\n",
    "        \n",
    "        image,density_map,gt_points = self._load_image_and_density_map(idx)\n",
    "        density_map = density_map.squeeze()\n",
    "        h,w = image.shape[0], image.shape[1]\n",
    "        #image = cv2.resize(image,(w, h))\n",
    "        \n",
    "        if self.method == 'train':\n",
    "            #h,w = image.shape[:2]\n",
    "            i,j = self._random_crop(h,w,self.crop_size,self.crop_size)\n",
    "            image = image[i:i+self.crop_size,j:j+self.crop_size]\n",
    "            density_map = density_map[i:i+self.crop_size,j:j+self.crop_size]\n",
    "            #print(density_map.shape)\n",
    "            #gt_points = gt_points - [j,i]\n",
    "            #mask = (gt_points[:,0] >=0 ) * (gt_points[:,0] <= self.crop_size) * (gt_points[:,1]>=0) * (gt_points[:,1]<=self.crop_size)\n",
    "            #gt_points = gt_points[mask]\n",
    "            density_map = cv2.resize(density_map,(self.crop_size//self.downsample,self.crop_size//self.downsample))\n",
    "            \n",
    "        else:\n",
    "            density_map = cv2.resize(density_map,(w//self.downsample,h//self.downsample))#,interpolation=cv2.INTER_NEAREST)\n",
    "            #density_map = density_map[1:-1,:]\n",
    "        \n",
    "        if self.transforms:\n",
    "            for tfms in self.transforms:\n",
    "                aug = tfms(**{\n",
    "                    'image': image,\n",
    "                    'mask': density_map,\n",
    "                    #'keypoints': gt_points\n",
    "                })\n",
    "                #image, density_map, gt_points = aug['image'], aug['mask'], aug['keypoints']\n",
    "                image, density_map = aug['image'], aug['mask'] # issue with previous keypoints (albumentation?)\n",
    "        return image, density_map*LOG_PARA, fn, gt_points\n",
    "    \n",
    "    def _random_crop(self, im_h, im_w, crop_h, crop_w):\n",
    "        res_h = im_h - crop_h\n",
    "        res_w = im_w - crop_w\n",
    "        i = random.randint(0, res_h)\n",
    "        j = random.randint(0, res_w)\n",
    "        return i, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d70edd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = glob(images + '/*.jpg')\n",
    "split = int(len(fp) * 0.8)\n",
    "\n",
    "\n",
    "train_dataset = Crop_Dataset(path=path,\n",
    "                             image_fnames=fp[:split],dmap_folder='/rf_GT_vehicle',\n",
    "                             #gt_folder='/annotation',\n",
    "                             transforms=[get_train_transforms(),get_train_image_only_transforms()],\n",
    "                             crop_size=448,\n",
    "                             downsample=1,\n",
    "                                )\n",
    "\n",
    "\n",
    "\n",
    "valid_dataset = Crop_Dataset(path=path,\n",
    "                             image_fnames=fp[split:],dmap_folder='/rf_GT_vehicle',\n",
    "                             #gt_folder='/annotation',\n",
    "                             transforms=[get_valid_trainsforms()],\n",
    "                             method='valid',\n",
    "                             #crop_size=784,\n",
    "                             downsample=1\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "376527ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-0-1-18 0 0 1\n"
     ]
    }
   ],
   "source": [
    "def dist_init(host_addr, rank, local_rank, world_size, port=23456):\n",
    "    host_addr_full = 'tcp://' + host_addr + ':' + str(port)\n",
    "    torch.distributed.init_process_group(\"gloo\", init_method=host_addr_full,\n",
    "                                         rank=rank, world_size=world_size)\n",
    "    assert torch.distributed.is_initialized()\n",
    "    \n",
    "def get_ip(iplist):\n",
    "        ip = iplist.split('[')[0] + iplist.split('[')[1].split('-')[0]\n",
    "        return ip\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rank = int(os.environ['SLURM_PROCID'])\n",
    "local_rank = int(os.environ['SLURM_LOCALID'])\n",
    "world_size = int(os.environ['SLURM_NTASKS'])\n",
    "iplist = os.environ['SLURM_JOB_NODELIST']\n",
    "#ip = get_ip(iplist)\n",
    "print(iplist, rank, local_rank, world_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dist_init(iplist, rank, local_rank, world_size, 23456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cea7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n",
    "val_sampler = torch.utils.data.distributed.DistributedSampler(valid_dataset, num_replicas=world_size, rank=rank)\n",
    "\n",
    "backbone = nn.Sequential(*list(timm.create_model('swin_base_patch4_window7_224', pretrained=True, num_classes=0).children())[:-2])\n",
    "act_cls = nn.ReLU\n",
    "idx = [1,3,21,23]\n",
    "\n",
    "\n",
    "def get_pad(im_sz, crop_sz=448,**kwargs):\n",
    "    h,w = im_sz\n",
    "    h_mul = h // crop_sz + 1\n",
    "    w_mul = w // crop_sz + 1\n",
    "    pad_h = (crop_sz * h_mul - h) % crop_sz\n",
    "    pad_w = (crop_sz * w_mul - w) % crop_sz\n",
    "    assert pad_h%2 == 0\n",
    "    assert pad_w%2 == 0\n",
    "    return pad_h//2, pad_w//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76661956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSELoss_MCNN(preds,targs):\n",
    "    return nn.MSELoss()(preds,targs)\n",
    "\n",
    "def MAELoss_MCNN(preds,targs,upsample):\n",
    "    return nn.L1Loss()((preds/LOG_PARA).sum(dim=[-1,-2])*upsample*upsample, (targs/LOG_PARA).sum(dim=[-1,-2])*upsample*upsample)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdbdf6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#opt_level ='O1' # apex\n",
    "\n",
    "class Fitter:\n",
    "    \n",
    "    def __init__(self, model, device, config):\n",
    "        self.config = config\n",
    "        self.epoch = 0\n",
    "\n",
    "        self.base_dir = f'/mnt/home/zpengac/USERDIR/count/drone_benchmark/{config.folder}'\n",
    "        if not os.path.exists(self.base_dir):\n",
    "            os.makedirs(self.base_dir)\n",
    "        \n",
    "        self.log_path = f'{self.base_dir}/log.txt'\n",
    "        self.best_summary_loss = 10**5\n",
    "\n",
    "        self.model = model\n",
    "        if opt.distribute and opt.local_rank != -1:\n",
    "            self.model.to(device)\n",
    "            self.model = DDP(self.model, device_ids=[opt.local_rank], output_device=opt.local_rank)\n",
    "        self.device = device\n",
    "\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ] \n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n",
    "        \n",
    "        #self.model, self.optimizer = amp.initialize(self.model,self.optimizer,opt_level=opt_level) # apex\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n",
    "        self.criterion = MSELoss_MCNN\n",
    "        self.metric = MAELoss_MCNN\n",
    "        self.log(f'Fitter prepared. Device is {self.device}')\n",
    "        \n",
    "        # self.iters_to_accumulate = 4 # gradient accumulation\n",
    "\n",
    "    def fit(self, train_loader, validation_loader):\n",
    "        for e in range(self.config.n_epochs):\n",
    "            train_sampler.set_epoch(e)\n",
    "            if self.config.verbose and dist.get_rank()== 0:\n",
    "                lr = self.optimizer.param_groups[0]['lr']\n",
    "                timestamp = datetime.utcnow().isoformat()\n",
    "                self.log(f'\\n{timestamp}\\nLR: {lr}')\n",
    "\n",
    "            t = time.time()\n",
    "            summary_loss, mae_loss = self.train_one_epoch(train_loader)\n",
    "            \n",
    "            if dist.get_rank()== 0:\n",
    "                self.log(f'[RESULT]: Train. Epoch: {self.epoch}, mse_loss: {summary_loss.avg:.8f}, time: {(time.time() - t):.5f}')\n",
    "                self.log(f'[RESULT]: Train. Epoch: {self.epoch}, mae_loss: {mae_loss.avg:.8f}, time: {(time.time() - t):.5f}')\n",
    "                self.save(f'{self.base_dir}/last-checkpoint.bin')\n",
    "\n",
    "            t = time.time()\n",
    "            summary_loss, mae_loss = self.validation(validation_loader)\n",
    "            \n",
    "            if dist.get_rank()== 0:\n",
    "                self.log(f'[RESULT]: Val. Epoch: {self.epoch}, mse_loss: {summary_loss.avg:.8f}, time: {(time.time() - t):.5f}')\n",
    "                self.log(f'[RESULT]: Val. Epoch: {self.epoch}, mae_loss: {mae_loss.avg:.8f}, time: {(time.time() - t):.5f}')\n",
    "            if summary_loss.avg < self.best_summary_loss:\n",
    "                self.best_summary_loss = summary_loss.avg\n",
    "                if dist.get_rank()== 0:\n",
    "                    self.model.eval()\n",
    "                    self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n",
    "                    for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-3]:\n",
    "                        os.remove(path)\n",
    "\n",
    "            if self.config.validation_scheduler:\n",
    "                self.scheduler.step(metrics=summary_loss.avg)\n",
    "\n",
    "            self.epoch += 1\n",
    "\n",
    "    def validation(self, val_loader):\n",
    "        self.model.eval()\n",
    "        summary_loss = AverageMeter()\n",
    "        mae_loss = AverageMeter()\n",
    "        t = time.time()\n",
    "        for step, (images, density_maps, fns, gt_pts) in enumerate(val_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0 and dist.get_rank()== 0:\n",
    "                    print(\n",
    "                        f'Val Step {step}/{len(val_loader)}, ' + \\\n",
    "                        f'mse_loss: {summary_loss.avg:.8f}, ' + \\\n",
    "                        f'mae_loss: {mae_loss.avg:.8f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            with torch.no_grad():\n",
    "                batch_size = images.shape[0]\n",
    "                #images = images.to(self.device).float()\n",
    "                images = images.cuda().float()\n",
    "                #density_maps = density_maps.to(self.device).float()\n",
    "                density_maps = density_maps.cuda().float()\n",
    "                \n",
    "                if pad_mode == True:\n",
    "                    _, _, h, w = images.shape\n",
    "                    pad_h, pad_w = get_pad((h, w),crop_size)\n",
    "                    images = F.pad(images,(pad_w,pad_w,pad_h-1,pad_h-1))\n",
    "                    density_maps = F.pad(density_maps,(pad_w,pad_w,pad_h,pad_h))\n",
    "                    bs, ch, h, w = images.shape\n",
    "                    m,n = int(h/crop_size), int(w/crop_size)\n",
    "                    loss,metric_loss = 0, 0\n",
    "                    \n",
    "                \n",
    "                with torch.cuda.amp.autocast(): #native fp16\n",
    "                    for i in range(m):\n",
    "                        for j in range(n):\n",
    "                            img_patches = images[:,:,crop_size*i:crop_size*(i+1),crop_size*j:crop_size*(j+1)]\n",
    "                            dmaps_patches = density_maps[:,:,crop_size*i:crop_size*(i+1),crop_size*j:crop_size*(j+1)]\n",
    "                            preds = self.model(img_patches)\n",
    "                            loss += self.criterion(preds,dmaps_patches)\n",
    "                            metric_loss += self.metric(preds,dmaps_patches,self.config.downsample)\n",
    "                mae_loss.update(metric_loss.detach().item(),batch_size)\n",
    "                summary_loss.update(loss.detach().item(), batch_size)\n",
    "            \n",
    "#             if step == 20:\n",
    "#                 break\n",
    "\n",
    "        return summary_loss, mae_loss\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        summary_loss = AverageMeter()\n",
    "        mae_loss = AverageMeter()\n",
    "        t = time.time()\n",
    "        for step, (images, density_maps, fns, gt_pts) in enumerate(train_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0 and dist.get_rank()== 0:\n",
    "                    print(\n",
    "                        f'Train Step {step}/{len(train_loader)}, ' + \\\n",
    "                        f'mse_loss: {summary_loss.avg:.8f}, ' + \\\n",
    "                        f'mae_loss: {mae_loss.avg:.8f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            \n",
    "            #images = images.to(self.device).float()\n",
    "            images = images.cuda().float()\n",
    "            batch_size = images.shape[0]\n",
    "            #density_maps = density_maps.to(self.device).float()\n",
    "            density_maps = density_maps.cuda().float()\n",
    "            \n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "#             with torch.cuda.amp.autocast(): #native fp16\n",
    "#                 preds = self.model(images)\n",
    "#                 loss = self.criterion(preds,density_maps)\n",
    "#                 metric_loss = self.metric(preds.detach(),density_maps.detach(),self.config.downsample)\n",
    "\n",
    "            if pad_mode == True:\n",
    "                #images = F.pad(images,(pad_w,pad_w,pad_h-1,pad_h-1))\n",
    "                #density_maps = F.pad(density_maps,(pad_w,pad_w,pad_h,pad_h))\n",
    "                #bs, ch, h, w = images.shape\n",
    "                images = F.pad(images,(t_pad_w,t_pad_w,t_pad_h,t_pad_h))\n",
    "                density_maps = F.pad(density_maps, (t_pad_w,t_pad_w,t_pad_h,t_pad_h))\n",
    "                m,n = int(448/crop_size), int(448/crop_size)\n",
    "                loss,metric_loss = 0, 0\n",
    "            with torch.cuda.amp.autocast(): #native fp16\n",
    "                for i in range(m):\n",
    "                    for j in range(n):\n",
    "                        img_patches = images[:,:,crop_size*i:crop_size*(i+1),crop_size*j:crop_size*(j+1)]\n",
    "                        dmaps_patches = density_maps[:,:,crop_size*i:crop_size*(i+1),crop_size*j:crop_size*(j+1)]\n",
    "                        preds = self.model(img_patches)\n",
    "                        loss += self.criterion(preds,dmaps_patches)\n",
    "                        metric_loss += self.metric(preds,dmaps_patches,self.config.downsample)\n",
    "            self.scaler.scale(loss).backward()\n",
    "            \n",
    "            # loss = loss / self.iters_to_accumulate # gradient accumulation\n",
    "            \n",
    "#             with amp.scale_loss(loss,self.optimizer) as scaled_loss: # apex\n",
    "#                 scaled_loss.backward()\n",
    "            #loss.backward()\n",
    "\n",
    "            \n",
    "            mae_loss.update(metric_loss.detach().item(),batch_size)\n",
    "            summary_loss.update(loss.detach().item(), batch_size)\n",
    "            \n",
    "            #self.optimizer.step()\n",
    "            self.scaler.step(self.optimizer) # native fp16\n",
    "            \n",
    "            if self.config.step_scheduler:\n",
    "                self.scheduler.step()\n",
    "            \n",
    "            self.scaler.update() #native fp16\n",
    "            \n",
    "                \n",
    "                \n",
    "#             if (step+1) % self.iters_to_accumulate == 0: # gradient accumulation\n",
    "\n",
    "#                 self.optimizer.step()\n",
    "#                 self.optimizer.zero_grad()\n",
    "\n",
    "#                 if self.config.step_scheduler:\n",
    "#                     self.scheduler.step()\n",
    "\n",
    "\n",
    "        return summary_loss, mae_loss\n",
    "    \n",
    "    def save(self, path):\n",
    "        self.model.eval()\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_summary_loss': self.best_summary_loss,\n",
    "            'epoch': self.epoch,\n",
    "            #'amp': amp.state_dict() # apex\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.best_summary_loss = checkpoint['best_summary_loss']\n",
    "        self.epoch = checkpoint['epoch'] + 1\n",
    "        \n",
    "    def log(self, message):\n",
    "        if self.config.verbose:\n",
    "            print(message)\n",
    "        with open(self.log_path, 'a+') as logger:\n",
    "            logger.write(f'{message}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc7bb33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_mode = True\n",
    "crop_size = 224\n",
    "#pad_h, pad_w = get_pad((1530,2720),crop_size)\n",
    "t_pad_h,t_pad_w = get_pad((448,448),crop_size)\n",
    "\n",
    "\n",
    "class TrainGlobalConfig:\n",
    "    num_workers =4\n",
    "    batch_size = 2\n",
    "    n_epochs = 120 \n",
    "    lr = 0.0002\n",
    "\n",
    "    folder = 'counTR_drone_vehicle'\n",
    "    downsample = 1\n",
    "\n",
    "    # -------------------\n",
    "    verbose = True\n",
    "    verbose_step = 1\n",
    "    # -------------------\n",
    "\n",
    "    # --------------------\n",
    "    step_scheduler = True  # do scheduler.step after optimizer.step\n",
    "    validation_scheduler = False  # do scheduler.step after validation stage loss\n",
    "\n",
    "    SchedulerClass = torch.optim.lr_scheduler.OneCycleLR\n",
    "    scheduler_params = dict(\n",
    "        max_lr=1e-4,\n",
    "        #total_steps = len(train_dataset) // 4 * n_epochs, # gradient accumulation\n",
    "        epochs=n_epochs,\n",
    "        steps_per_epoch=int(len(train_dataset) / (batch_size*torch.cuda.device_count())),\n",
    "        pct_start=0.2,\n",
    "        anneal_strategy='cos', \n",
    "        final_div_factor=10**5\n",
    "    )\n",
    "    \n",
    "#     SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "#     scheduler_params = dict(\n",
    "#         mode='min',\n",
    "#         factor=0.5,\n",
    "#         patience=1,\n",
    "#         verbose=False, \n",
    "#         threshold=0.0001,\n",
    "#         threshold_mode='abs',\n",
    "#         cooldown=0, \n",
    "#         min_lr=1e-8,\n",
    "#         eps=1e-08\n",
    "#     )\n",
    "net = CounTR(backbone,act_cls=act_cls,\n",
    "             imsize=224,layer_idx=idx,\n",
    "             self_attention=False).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6605f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs, dmaps, fns, gt_points = zip(*batch)\n",
    "    imgs = torch.stack(imgs)\n",
    "    dmaps = torch.stack(dmaps).unsqueeze(1)\n",
    "    return imgs,dmaps,fns,gt_points\n",
    "\n",
    "def run_training():\n",
    "    device = torch.device('cuda:0')\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=train_sampler,\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size=1,\n",
    "        num_workers=1,\n",
    "        shuffle=False,\n",
    "        sampler=valid_sampler,#SequentialSampler(valid_dataset),\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n",
    "#     fitter.load(f'{fitter.base_dir}/last-checkpoint.bin')\n",
    "    fitter.fit(train_loader, val_loader)\n",
    "\n",
    "#run_training() # Comment this line when inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "813218fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=1,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    "    sampler=SequentialSampler(valid_dataset),\n",
    "    #sampler=val_sampler,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39df658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_net = CounTR(backbone,act_cls=act_cls,\n",
    "#             imsize=224,layer_idx=idx,\n",
    "#             self_attention=False).cuda()\n",
    "test_net = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da2a830d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CounTR(\n",
       "  (backbone): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): PatchEmbed(\n",
       "        (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "      (2): Sequential(\n",
       "        (0): BasicLayer(\n",
       "          dim=128, input_resolution=(56, 56), depth=2\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            input_resolution=(56, 56), dim=128\n",
       "            (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicLayer(\n",
       "          dim=256, input_resolution=(28, 28), depth=2\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            input_resolution=(28, 28), dim=256\n",
       "            (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): BasicLayer(\n",
       "          dim=512, input_resolution=(14, 14), depth=18\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (5): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (6): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (7): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (8): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (9): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (10): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (11): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (12): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (13): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (14): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (15): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (16): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (17): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            input_resolution=(14, 14), dim=512\n",
       "            (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): BasicLayer(\n",
       "          dim=1024, input_resolution=(7, 7), depth=2\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvLayer(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (1): ConvLayer(\n",
       "        (0): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (1): MyUnetBlock(\n",
       "      (shuf): PixelShuffle_ICNR(\n",
       "        (0): ConvLayer(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "        (1): PixelShuffle(upscale_factor=2)\n",
       "      )\n",
       "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): ConvLayer(\n",
       "        (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): MyUnetBlock(\n",
       "      (shuf): PixelShuffle_ICNR(\n",
       "        (0): ConvLayer(\n",
       "          (0): Conv2d(1536, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "        (1): PixelShuffle(upscale_factor=2)\n",
       "      )\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): ConvLayer(\n",
       "        (0): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (0): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): MyUnetBlock(\n",
       "      (shuf): PixelShuffle_ICNR(\n",
       "        (0): ConvLayer(\n",
       "          (0): Conv2d(1280, 2560, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "        (1): PixelShuffle(upscale_factor=2)\n",
       "      )\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): ConvLayer(\n",
       "        (0): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (0): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (4): MyUnetBlock(\n",
       "      (shuf): PixelShuffle_ICNR(\n",
       "        (0): ConvLayer(\n",
       "          (0): Conv2d(896, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "        (1): PixelShuffle(upscale_factor=2)\n",
       "      )\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): ConvLayer(\n",
       "        (0): Conv2d(576, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (5): PixelShuffle_ICNR(\n",
       "      (0): ConvLayer(\n",
       "        (0): Conv2d(288, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (1): PixelShuffle(upscale_factor=2)\n",
       "    )\n",
       "    (6): UpScaleOrig()\n",
       "    (7): ConvLayer(\n",
       "      (0): Conv2d(288, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(f'/mnt/home/zpengac/USERDIR/count/drone_benchmark/counTR_drone_vehicle/best-checkpoint-099epoch.bin')\n",
    "test_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "test_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf72a066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/988 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor to have CPU Backend, but got tensor with CUDA Backend (while checking arguments for batch_norm_cpu)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-60deac68d6ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mimg_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcrop_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcrop_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcrop_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcrop_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mdmaps_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdensity_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcrop_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcrop_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcrop_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcrop_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_patches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mpredc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mdmapc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdmaps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.Miniconda3/envs/f4774e49c9ffe87fb0928ec97f8ff682/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.Miniconda3/envs/f4774e49c9ffe87fb0928ec97f8ff682/lib/python3.7/site-packages/torch/cuda/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/USERDIR/count/drone_benchmark/nbs/counTR.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0msz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mni\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.Miniconda3/envs/f4774e49c9ffe87fb0928ec97f8ff682/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.Miniconda3/envs/f4774e49c9ffe87fb0928ec97f8ff682/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.Miniconda3/envs/f4774e49c9ffe87fb0928ec97f8ff682/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.Miniconda3/envs/f4774e49c9ffe87fb0928ec97f8ff682/lib/python3.7/site-packages/torch/cuda/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/USERDIR/count/drone_benchmark/nbs/counTR.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, up_in)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mssh\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mup_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mup_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mup_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nearest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mcat_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mup_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.Miniconda3/envs/f4774e49c9ffe87fb0928ec97f8ff682/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.Miniconda3/envs/f4774e49c9ffe87fb0928ec97f8ff682/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             self.weight, self.bias, bn_training, exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.Miniconda3/envs/f4774e49c9ffe87fb0928ec97f8ff682/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2056\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   2057\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2058\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2059\u001b[0m     )\n\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor to have CPU Backend, but got tensor with CUDA Backend (while checking arguments for batch_norm_cpu)"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "#from skimage.metrics import structural_similarity as ssim\n",
    "#from skimage.metrics import peak_signal_noise_ratio as pnsr\n",
    "\n",
    "pre_count = []\n",
    "gt_count = []\n",
    "gt_points = []\n",
    "#avg_ssim = AverageMeter()\n",
    "#avg_pnsr = AverageMeter()\n",
    "for images, density_maps, fns, points in tqdm(val_loader):\n",
    "    batch_size = images.shape[0]\n",
    "    #images = images.to(self.device).float()\n",
    "    images = images.cuda().float()\n",
    "    #density_maps = density_maps.to(self.device).float()\n",
    "    density_maps = density_maps.cuda().float()\n",
    "\n",
    "    _, _, h, w = images.shape\n",
    "    pad_h, pad_w = get_pad((h, w),crop_size)\n",
    "    images = F.pad(images,(pad_w,pad_w,pad_h,pad_h))\n",
    "    density_maps = F.pad(density_maps,(pad_w,pad_w,pad_h,pad_h))\n",
    "    bs, ch, h, w = images.shape\n",
    "    m,n = int(h/crop_size), int(w/crop_size)\n",
    "    predc, dmapc = 0, 0\n",
    "\n",
    "    with torch.cuda.amp.autocast(): #native fp16\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                img_patches = images[:,:,crop_size*i:crop_size*(i+1),crop_size*j:crop_size*(j+1)]\n",
    "                dmaps_patches = density_maps[:,:,crop_size*i:crop_size*(i+1),crop_size*j:crop_size*(j+1)]\n",
    "                preds = test_net(img_patches)\n",
    "                predc += preds.sum(dim=[-1,-2]).detach().cpu().numpy()\n",
    "                dmapc += dmaps.sum(dim=[-1,-2]).detach().cpu().numpy()\n",
    "    \n",
    "    pre_count.append(predc)\n",
    "    \n",
    "    gt_count.append(dmapc)\n",
    "    \n",
    "    #gt_p = []\n",
    "    #for p in points:\n",
    "    #    gt_p.append(len(p))\n",
    "    #gt_points.extend(gt_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f56e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(pre_count,gt_count)\n",
    "mse = mean_squared_error(pre_count,gt_count)\n",
    "nae = mae * len(pre_count) / np.sum(gt_count)\n",
    "\n",
    "def count_parameters_in_MB(model):\n",
    "    return np.sum(np.prod(v.size()) for name, v in model.named_parameters() if \"auxiliary\" not in name) / 1e6\n",
    "\n",
    "print(f'#Paras: {count_parameters_in_MB(test_net)}')\n",
    "print(f'MAE: {mae}, MSE: {mse}, NAE: {nae}')\n",
    "#print(f'SSIM: {avg_ssim.avg}, PNSR: {avg_pnsr.avg}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
